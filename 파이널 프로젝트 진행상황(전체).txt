파이널 프로젝트 진행상황


프로젝트 환경
	Window 10 pro
		apache tomcat 8.5
		eclipse ide
			sts 4
			maven rep
				mybatis
				spring-mongodb
				spring-spark-2.10
				jsoup(차차 제거할 예정)
				spring-core .....
				hadoop-core .....
			pydev
		jdk 1.8_192	
		python 3.7.2
			pymongo
			asyncio
			multiprocessing
			cx_Oracle(debuging)
			bs4
			lxml
			Selenium
			gensim(test)
			matplotlib(test)
			numpy
			pandas
			py4j
			pyspark 2.4.3
		oracle 10g
			ojdbc 6
			oci.dll from oracle 11g
		mongodb 4.0.8
		VirtualBox6++
			centos6.5 : hadoop01
				apache hadoop 2.6.5
			centos6.5 : hadoop02
				apache hadoop 2.6.5
			centos6.5 : hadoop03
				apache hadoop 2.6.5
			centos6.5 : hadoop04(master)
				apache hadoop 2.6.5
				apache sqoop 1.4.7
				apache flume 1.8.0
				apache hive  2.3.5
				apache spark 2.4.3
							
검색엔진
	시나리오
		A.
			1. 뉴스 데이터를 긁어 적절한 전처리 후 mongodb 에 입력
			2. 입력된 데이터를 인출해 조사, 함수, 전처리 후 카운트를 통해 mr --> 2.1 merge 후 hdfs로 이동시킬 계획
			3-a. hdfs 에 있는 단어 단어간 tf 결과 large table 을 통해 문서- 문서간 large table 구축  
			or
			3-b. large table을 만들지 않고 뉴스 번호에 대한 요청이 들어왔을때 full-scan 한번으로 연산을 끝내고 리턴
			4. 단어 검색 요청이 들어왔을때 단어-단어간 테이블을 통해 일차다원함수 계수행렬을 만든 뒤 문서에 페이지 랭크 메기기
			5. 리턴한 문서중 top-n개의 문서와 일정 이상의 유사도를 보이는 문서 리턴 리스트에서 제거하기
		B.
			1. dart의 재무재표에 적혀있는 수치들을 동적으로 크롤링해 mongodb에 저장
			2. 간단한 그래프를 그릴 수 있도록 폼 지정
		
		C.
			1. 기존 자바 Jsoup으로 받아 RDBMS에 넣던 API 데이터들을 python bs4 -> pymongo 를 통해 bson으로 관리
				1-1. dart 기업정보, 기업상세
				1-2. 워크넷 공채정보, 공체상세
		
		
	구현된 상황
		
		A-1, A-2, A-3-a'(mongodb와 python으로 간략한 형태만 나와있는 상태)
		
	구현되지 않은 상황
		
		A-2.1, A-3-a or A-3-b,A-4,A-5 B-2, C-1-1, C-1-2
		
		구현상세계획
			
			A-2.1
			mr한뒤 mongodb, local file system 에 저장해두었던 large table을 hdfs에 대신 저장
			
			A-3-a or A-3-b
			pyspark를 통해 hdfs에 있는 large table을 rdd로 접근
			row단위 연산을 병령처리하여 새로운 large table 구축 후 hdfs에 저장(두 테이블 모두 사용한다)
			혹은 서버 요청이 들어왔을때 spring-spark에서 한줄씩 연산하여 가져오기
			(large table을 만들었다면 spring-spark 에서는 줄을 선택한 후 읽은 뒤 리스트 형으로 리턴 혹은 
			distinct 연산만 실행할 계획)
			
			A-4
			단어 검색 request가 controller에 들어왔을때 hdfs에서 단어-단어간 분석테이블에서 검색단어를 filter해서 추출
			sort해서 상위 n개로 slice한뒤 스코어링 함수 호출 
			spring-mongo의 news_analyze 컬렉션에서 값들을 hashmap으로 추출, 반복문 에서 상위 n개의 키워드와 맞는 단어 찾고 스코어에 + 연산
			연산이 끝난 뒤 hashmap의 value를 score로 변경, values 기준으로 정렬한뒤 keys를리턴(biz에서 실행)
			
			A-5
			A-4에서 리턴한 리스트중 상위 N개의 키를 리스트로 저장 후 HDFS의 문서-문서간 분석테이블에서 맞는 row를 뽑아온다
			(row,col)(유사도)의 값이 일정 값이 넘는 뉴스 번호(키) 를 리스트로 저장한 뒤 리턴하기로한 keys(arraylist) 에서 diff 연산
			후 controller에 전달
			
			
			B-2
			간단한 코드
			
			C-1-*
			간단한 코드
			
			
		
	우선순위
		A > C > B
	
	미구현 의사코드
	A-2.1
	import pyspark
	class pymatrixmk:
	~~~~~~~~~~~~~~~~~~~~~~
	~~~~~~~~~~~~~~~~~~~~~~
	
	df_t.write.csv("hdfs://192.168.56.104:9000/compath/"+date+ "/" )
	
	날짜별 merge
	meg = sc.textFile("hdfs://192.168.56.104:9000/compath/tf_idf/res.csv")
	for i in date_list
		status = fs.globStatus(Path('hdfs://192.168.56.104:9000/compath/tf_idf/*.csv'))
	#파일이 2개 이상 생기는 경우를 위해 파일 리스트로 받기
		for i in status:
			meg = join(meg,i,how="outer",leftindex=True,rightindex=True)
		
		
	meg.write.csv("hdfs://192.168.56.104:9000/compath/tf_idf/res.csv",mod="overwrite")	
	
	
	A-3-a or A-3-b
	meg = sc.textFile("hdfs://192.168.56.104:9000/compath/tf_idf/res.csv")
	res = def_make_cos_sim(meg)
	res.write.csv("hdfs://192.168.56.104:9000/compath/cos_sim/res.csv",mod="overwrite")

	A-4
	file = hdfs.fs.("hdfs://192.168.56.104:9000/compath/tf_idf/res.csv")
		.fiilter(function(@lambda){lambda x : x == req.getparameter("search_word")}).sort(asdasd).slice(0,n)
	
	list_key = file.getheader()
	map indexmap = new hashmap()
	for(stirng i : list_key) {
		indexmap.put(i,file[req.getparameter("search_word"),i])
	}
	
	score(map indexmap){
		res = springmongo.con("localhost",27777).collection("news_analyze").find({},{"news_number" : 1 , "data" : 1})
		ks = indexmap.keys()
		map newmap = new hashmap()
		for(map i : res){
			kres = i.data.keys().retainAll(ks)
			int score = kres * indexmap.valuse()#psuedo ops
			newmap.put(i.news_number,score)
		}
		
		newmap.sortbyvalues(Sort.ASCENDING)
		
		return newmap
		
	}
		
	A-5
	dist(map newmap){
		lists = newmap.keys()
		file = hdfs.fs.("hdfs://192.168.56.104:9000/compath/cos_sim/res.csv")
		.fiilter(function(@lambda){lambda x : x == anyof lists elements }).sort(asdasd).slice(0,n)
		sim_news = file.cols() //arraylist
		new_list = lists.removeAll(sim_news)
		
		for( i : newmap){
			if i.key().in(new_list){
				continue
			}else {
				newmap.remove(i.key())
			}
			
		}
		
		return newmap
		
		
	}

로그 분석 시스템 (랜덤포레스트) by CPcloser

		A. 
			구현 목표 : 
				로그를 가지고 각 개인이 회원가입시 입력했던 인적사항, 본 뉴스들의 카테고리, 주요 500개 단어 노출 빈도 등을 기반으로 어떠한 기업을 선호하는지에 대해 분류 모델을 작성
			          	해당 모델을 토대로 개인의 로그를 통해서 도출된 기업을 추천 노출
		B. 
			구현 시나리오

			1. 개념적 테이블 생성
				Y (관심기업으로 등록한 기업) = X1 그룹 (회원가입시 기재하였던 연령, 학력 등의 기재사항)  + X2그룹 (자사 홈페이지를 통해 고객이 읽었던 뉴스들의 개별 카테고리)+ X3 그룹(많이 노출된 상위 500개 단어에 노출된 횟수)

			2. 해당 테이블에 채울 데이터를 csv 혹은 DataFrame으로 RandomForest 실행

			3. 모델을 통해 도출되는 회사를 노출

		C.
			구현 과정 
	
				1. (연습 데이터 (iris) 을 통해 랜덤 포레스트 코딩)
				1-1. sklearn 패키지를 통해서 현재 랜덤 포레스트로 모델 구현 (完)
				1-2. sklearn 패키지를 통해서 구현된 모델의 정확성 출력 (完)
				1-3. sklearn 패키지를 이용해 해당 모델을 통해서 변수들이 입력되었을 때 결과값 도출 (미완)
 
				2-1 X1 그룹을 가진 dataframe 구축 (미완)
				2-2 X2 그룹을 가진 dataframe 구축 (미완)
				2-3 X2 그룹을 가진 dataframe 구축 (미완)


				3. 실제 로그 데이터와 랜덤 포레스트 코딩 결합


				1-1. 
					import matplotlib.pyplot as plt
					from sklearn import datasets
					import numpy as np
					import pandas as pd
					import seaborn as sns
					from sklearn.model_selection._validation import cross_val_score

					# 사용 데이터 = iris
					iris = datasets.load_iris()

					# Dataframe 화
					df = pd.DataFrame(iris.data, columns=iris.feature_names)

					df['species'] = np.array([iris.target_names[i] for i in iris.target])
					print(df)
					#데이터 펼쳐보기
					#sns.pairplot(df, hue='species')
					#plt.show()


					# train data 나누기

					from sklearn.model_selection import train_test_split
					X_train, X_test, y_train, y_test = train_test_split(df[iris.feature_names], iris.target, test_size=0.3, stratify=iris.target, random_state=123456)


					# Scikit-learn을 사용해 모델
					from sklearn.ensemble import RandomForestClassifier

					rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=123456)
					rf.fit(X_train, y_train)

					print(rf)

				1.2
					#모델을 가지고 test 세트를 분류

					from sklearn.metrics import accuracy_score

					predicted = rf.predict(X_test)
					accuracy = accuracy_score(y_test, predicted)

					print(f'Out-of-bag score estimate: {rf.oob_score_:.3}')
					print(f'Mean accuracy score: {accuracy:.3}')

					#히트맵 시각화

					from sklearn.metrics import confusion_matrix
					print("================================")

					cm = pd.DataFrame(confusion_matrix(y_test, predicted), columns=iris.target_names, index=iris.target_names)
					sns.heatmap(cm, annot=True)
					plt.show()

로그 수집 및 HDFS 적재 (후에 날짜별/혹은 유저별 파티셔닝)

	A-1. 로그 수집
		1. comp / news / recruit / sbook.log파일
			: 기업/뉴스/채용공고/스크랩북 디테일을 유저가 확인 시 
			  확인 시간, ip, id(혹은 sessionID), 기업번호/뉴스url/채용공고번호/스크랩북번호를 저장 (완료)
		2. comp / news / recruit / sbook -like.log
			: 유저가 관심 버튼을 누를 시 
			  확인 시간, ip, id(혹은 sessionID), 기업번호/뉴스url/채용공고번호/스크랩북번호를 저장 (완료)
		3. sb-url.log 스크랩북 작성에 참고한 기사의 url 수집
			: 스크랩북을 작성할 때 뉴스 기사에서 스크랩 에디터에 복사/붙여넣기 할 시
			  복사 할 때 클립보드에 url을 심고, 붙여넣기 할 때 url을 가져와서 저장
	A-2. 로그 적재
		flume apender로 클러스터 마스터인 hadoop02에 연결 후 hdfs에 적재
		성공 후 hive로 테이블 생성, 필요한 key별로 파티셔닝
	B. 진행 상황
		A-1-1 comp / news / recruit / sbook.log (완료)
		A-1-2 comp / news / recruit / sbook -like.log (미완)
		A-1-3 sb-url.log (미완)
		A-2 flume appender (미완) 설정 방법을 찾는 중...


